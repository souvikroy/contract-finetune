# Fine-tuning Configuration Template

# Base model
base_model: "Qwen/Qwen2.5-7B-Instruct"

# Output directory
output_dir: "./checkpoints/qwen-legal-lora"

# LoRA Configuration
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05

# Training Configuration
num_epochs: 3
batch_size: 4
gradient_accumulation_steps: 4
learning_rate: 2.0e-4
warmup_steps: 100
max_length: 2048

# Data Configuration
train_ratio: 0.8
val_ratio: 0.1
augment_data: true

# Training Options
use_qlora: false  # Set to true for 4-bit quantization
fp16: true
bf16: false
gradient_checkpointing: true
merge_adapters: true

# Logging
logging_steps: 100
save_steps: 500
eval_steps: 500

# Data paths
contract_path: "./RFP_parsed.json"
training_data_path: "./data/training/legal_qa.jsonl"
